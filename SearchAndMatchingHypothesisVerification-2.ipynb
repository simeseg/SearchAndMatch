{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    obb = source_temp.get_oriented_bounding_box()\n",
    "    obb.color = (0,1,0) #obbBounding box is green\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp, obb],\n",
    "                                      zoom=0.4559,\n",
    "                                      front=[0.6452, -0.3036, -0.7011],\n",
    "                                      lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                      up=[-0.2779, -0.9482, 0.1556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal radius, feature radius\n",
    "def getISSKeypoints(pcd):\n",
    "    return o3d.geometry.keypoint.compute_iss_keypoints(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract geometric features\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size, visualize = False, iss = True):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    \n",
    "    keypoints = pcd.voxel_down_sample(voxel_size)\n",
    "    \n",
    "    if iss:\n",
    "        keypoints = getISSKeypoints(keypoints) #\n",
    "    \n",
    "    radius_normal = voxel_size * 3\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    keypoints.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "    keypoints.orient_normals_consistent_tangent_plane(k = 10)\n",
    "    \n",
    "    if visualize:\n",
    "        o3d.visualization.draw_geometries([keypoints])\n",
    "    \n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        keypoints,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=30))\n",
    "    return keypoints, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add noise to pointcloud\n",
    "def noised(pcd):\n",
    "    arr = np.array([pt + np.random.normal(0.1, 0.5 , 3) for pt in list(np.asarray(pcd.points))])\n",
    "    out = o3d.geometry.PointCloud()\n",
    "    out.points = o3d.utility.Vector3dVector(arr)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input \n",
    "def read_pcds(filename_model, filename_scene):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "    source = o3d.io.read_point_cloud(filename_model)\n",
    "    target = o3d.io.read_point_cloud(filename_scene)\n",
    "    trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "                             [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    \n",
    "    #source.transform(trans_init)\n",
    "    #draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#correspondence matching\n",
    "#query= source match=target\n",
    "\n",
    "class correspondence():\n",
    "    def __init__(self, query, match, distance):\n",
    "        self.query = query\n",
    "        self.match = match\n",
    "        self.distance = distance\n",
    "\n",
    "class findCorrespondences():\n",
    "    def __init__(self, source_down, target_down, source_fpfh, target_fpfh, nn, dist_thresh):\n",
    "        self.source_down = source_down\n",
    "        self.target_down = target_down\n",
    "        self.source_fpfh = np.array(source_fpfh.data)\n",
    "        self.target_fpfh = np.array(target_fpfh.data)\n",
    "        self.source_tree = o3d.geometry.KDTreeFlann(self.source_fpfh)\n",
    "        self.target_tree = o3d.geometry.KDTreeFlann(self.target_fpfh)        \n",
    "        self.dist_thresh = dist_thresh\n",
    "        self.nn = nn\n",
    "        self.correspondences = []\n",
    "        self.points = []\n",
    "        self.lines  = []\n",
    "        self.colors = []\n",
    "        \n",
    "    def squaredDistance(self, arr1, arr2):\n",
    "        return np.sqrt(np.sum((arr1 - arr2)**2))\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        for i in range(len(self.source_down.points)):\n",
    "\n",
    "            #find the nearest neighbour(in descriptor space) in the target\n",
    "            [k, idx, sqDist] = self.target_tree.search_knn_vector_xd(self.source_fpfh[:,i], self.nn)\n",
    "            output_points_i = self.target_fpfh[:,idx]\n",
    "           \n",
    "            for n in range(k):\n",
    "                output_point_i = output_points_i[:,n]\n",
    "                dist_i = np.sqrt(sqDist[n])\n",
    "                #dist = self.squaredDistance(source_i[...,np.newaxis], output_i)\n",
    "\n",
    "                if dist_i<self.dist_thresh:\n",
    "                    #print(dist)\n",
    "                    ln = len(self.correspondences)\n",
    "                    corrs = correspondence(i, idx[n], dist_i)\n",
    "                    self.correspondences.append(corrs)\n",
    "                    #print(corrs.query, corrs.match)\n",
    "                    target_pt = self.target_down.points[corrs.match]\n",
    "                    source_pt = self.source_down.points[corrs.query]\n",
    "                    self.points.append(source_pt)\n",
    "                    self.points.append(target_pt)\n",
    "                    self.lines.append([2*ln, 2*ln+1])\n",
    "                    self.colors.append([1,0,1])\n",
    "\n",
    "        return self.correspondences, self.points     \n",
    "            \n",
    "    def visualize(self):\n",
    "        line_set = o3d.geometry.LineSet()\n",
    "        line_set.points = o3d.utility.Vector3dVector(self.points)\n",
    "        line_set.lines  = o3d.utility.Vector2iVector(self.lines)\n",
    "        line_set.colors = o3d.utility.Vector3dVector(self.colors)\n",
    "        o3d.visualization.draw_geometries([self.source_down, self.target_down, line_set])    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class correspondence_rejection_sample_consensus():\n",
    "    __slots__ = ('inlier_threshold_', 'max_iterations_', 'input_', 'input_transformed_', 'target_', 'refine_', 'save_inliers_',\n",
    "                'best_transformation_', 'inlier_indices_')\n",
    "    \n",
    "    def __init__(self, source, target):\n",
    "        \n",
    "        self.inlier_threshold_ = 0.005\n",
    "        self.max_iterations_ = 1000\n",
    "        self.refine_ = False\n",
    "        self.save_inliers_ = False\n",
    "        self.setInputSource(source)\n",
    "        self.setInputTarget(target)\n",
    "        \n",
    "    def setInputSource(self, cloud):\n",
    "        self.input_ = cloud\n",
    "    \n",
    "    def setInputTarget(self, cloud):\n",
    "        self.target_ = cloud\n",
    "    \n",
    "    def requiresSourcePoints(self):\n",
    "        return True\n",
    "    \n",
    "    def setSourcePoints(self, cloud2):\n",
    "        self.setInputSource(cloud2)\n",
    "        \n",
    "    def requiresTargetPoints(self):\n",
    "        return True\n",
    "    \n",
    "    def setTargetPoints(self, cloud2):\n",
    "        self.setInputTarget(cloud2)\n",
    "        \n",
    "    def setInlierThreshold(self, threshold):\n",
    "        self.inlier_threshold_ = threshold\n",
    "    \n",
    "    def setMaximumIterations(self, max_iterations):\n",
    "        self.max_iterations_ = max_iterations\n",
    "        \n",
    "    def setRefineModel(self, refine):\n",
    "        self.refine_ = refine\n",
    "        \n",
    "    def setSaveInliers(self, s):\n",
    "        self.save_inliers_ = s \n",
    "        \n",
    "    def applyRejection(slef, correspondences):\n",
    "        self.getRemainingCorrespondences(self.input_correspondences_, correspondences)\n",
    "    \n",
    "    def getVec2iVec(self, correspondences):\n",
    "        arr = np.array([[corresp.query, corresp.match] for corresp in correspondences])\n",
    "        return o3d.utility.Vector2iVector(arr)\n",
    "    \n",
    "    def getRemainingCorrespondences(self, original_correspondences):\n",
    "        remaining_correspondences = []\n",
    "        \n",
    "        if not self.input_: \n",
    "            print(\"no input cloud was given \\n\")\n",
    "            return\n",
    "        if not self.target_:\n",
    "            print(\"no target cloud was given \\n\")\n",
    "            return\n",
    "        if self.save_inliers_:\n",
    "            self.inlier_indices_.clear()\n",
    "            \n",
    "        nr_correspondences = len(original_correspondences)\n",
    "        \n",
    "        source_indices = []\n",
    "        target_indices = [] \n",
    "        \n",
    "        #copy the query-match indices\n",
    "        for i in range(nr_correspondences):\n",
    "            source_indices.append(original_correspondences[i].query)\n",
    "            target_indices.append(original_correspondences[i].match)\n",
    "            \n",
    "        #From the set of correspondences found, attempt to remove outliers\n",
    "        #create the registration model\n",
    "        correspVec2iVec = self.getVec2iVec(original_correspondences)\n",
    "        ransac = o3d.pipelines.registration.registration_ransac_based_on_correspondence(self.input_,\n",
    "        self.target_, correspVec2iVec, self.inlier_threshold_ )\n",
    "        #print(\"inlier threshold \", self.inlier_threshold_)\n",
    "        \n",
    "        #print(ransac.transformation)\n",
    "        if(ransac.transformation.all == None):\n",
    "            remaining_correspondences = original_correspondences\n",
    "            self.best_transformation_ = np.eye(4)\n",
    "            return\n",
    "        \n",
    "        #get inliers\n",
    "        inliers = np.asarray(ransac.correspondence_set).tolist()\n",
    "        \n",
    "        if len(inliers)<3: \n",
    "            remaining_correspondences = original_correspondences\n",
    "            self.best_transformation_ = np.eye(4)\n",
    "            return\n",
    "        \n",
    "        index_to_correspondence = {} \n",
    "        for i in range(nr_correspondences):\n",
    "            index_to_correspondence[original_correspondences[i].query] = i \n",
    "\n",
    "        remaining_correspondences = remaining_correspondences[:len(inliers)]\n",
    "        for i in range(len(inliers)):\n",
    "            remaining_correspondences.append(original_correspondences[index_to_correspondence[inliers[i][0]]])\n",
    "            \n",
    "        if self.save_inliers_:\n",
    "            for inlier in inliers:\n",
    "                self.inlier_indices_.append(index_to_correspondence[inlier])\n",
    "        \n",
    "        self.best_transformation_ = ransac.transformation \n",
    "        \n",
    "        return remaining_correspondences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class geometricConsistency():\n",
    "    __slots__ = ('gc_threshold_', 'gc_size_', 'input_', 'scene_', 'model_scene_corrs_', 'found_transformations_')\n",
    "    \n",
    "    def __init__(self, source, target, correspondences):\n",
    "        self.input_ = source\n",
    "        self.scene_ = target\n",
    "        self.model_scene_corrs_ = correspondences\n",
    "        self.found_transformations_ = []\n",
    "        \n",
    "    def norm(self, arr):\n",
    "        #return np.linalg.norm(arr)\n",
    "        return np.sqrt(np.sum(arr*arr))\n",
    "        \n",
    "    def setGCThreshold(self, gc_threshold):\n",
    "        self.gc_threshold_ = gc_threshold\n",
    "        \n",
    "    def setGCSize(self, gc_size):\n",
    "        self.gc_size_ = gc_size\n",
    "    \n",
    "    def clusterCorrespondences(self, model_instances):\n",
    "        model_instances.clear()\n",
    "        self.found_transformations_.clear()\n",
    "        \n",
    "        if not self.model_scene_corrs_:\n",
    "            print('Correspondences not set!')\n",
    "\n",
    "        sorted_corrs = self.model_scene_corrs_\n",
    "        sorted_corrs.sort(key = lambda corrs: corrs.distance)\n",
    "        self.model_scene_corrs_ = sorted_corrs\n",
    "        \n",
    "        consensus_set = []\n",
    "        taken_corresps = [False for i in self.model_scene_corrs_]\n",
    "        \n",
    "        dist_ref, dist_trg = [], []\n",
    "        \n",
    "        temp_scene_cloud_ptr = self.scene_\n",
    "        \n",
    "        corr_rejector = correspondence_rejection_sample_consensus(self.input_, self.scene_)\n",
    "        corr_rejector.setMaximumIterations(10000)\n",
    "        corr_rejector.setInlierThreshold(self.gc_size_)\n",
    "        corr_rejector.setInputSource(self.input_)\n",
    "        corr_rejector.setInputTarget(temp_scene_cloud_ptr)\n",
    "        \n",
    "        for i in range(len(self.model_scene_corrs_)):\n",
    "            if taken_corresps[i]:\n",
    "                continue\n",
    "                \n",
    "            consensus_set.clear()\n",
    "            consensus_set.append(i)\n",
    "            \n",
    "            for j in range(len(self.model_scene_corrs_)):\n",
    "                if (j!=i and not taken_corresps[j]):\n",
    "                    \n",
    "                    is_a_good_candidate = True\n",
    "                    \n",
    "                    for k in consensus_set:\n",
    "                        scene_index_k = self.model_scene_corrs_[k].match\n",
    "                        model_index_k = self.model_scene_corrs_[k].query\n",
    "                        scene_index_j = self.model_scene_corrs_[j].match\n",
    "                        model_index_j = self.model_scene_corrs_[j].query\n",
    "\n",
    "                        scene_point_k = self.scene_.points[scene_index_k]\n",
    "                        model_point_k = self.input_.points[model_index_k]\n",
    "                        scene_point_j = self.scene_.points[scene_index_j]\n",
    "                        model_point_j = self.input_.points[model_index_j]\n",
    "                        \n",
    "                        dist_ref = scene_point_k - scene_point_j\n",
    "                        dist_trg = model_point_k - model_point_j\n",
    "                        \n",
    "                        distance = np.abs(self.norm(dist_ref) - self.norm(dist_trg))\n",
    "\n",
    "                        if (distance > self.gc_size_):\n",
    "                            is_a_good_candidate = False\n",
    "                            break\n",
    "                            \n",
    "                    if is_a_good_candidate:\n",
    "                        consensus_set.append(j)\n",
    "                        \n",
    "            if len(consensus_set) > self.gc_threshold_:\n",
    "                temp_corrs = []\n",
    "                #print(len(self.consensus_set))\n",
    "                for j in consensus_set:\n",
    "                    temp_corrs.append(self.model_scene_corrs_[j])\n",
    "                    taken_corresps[j] = True \n",
    "                    \n",
    "                filtered_corrs = corr_rejector.getRemainingCorrespondences(temp_corrs)\n",
    "                self.found_transformations_.append(corr_rejector.best_transformation_)\n",
    "                model_instances.append(filtered_corrs)\n",
    "                \n",
    "    def recognize(self, transformations, clustered_Corrs):\n",
    "        self.clusterCorrespondences(clustered_Corrs)\n",
    "        transformations = self.found_transformations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alignment():\n",
    "    def __init__(self, results_ransac, source, target, distance_threshold):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.results_ransac = [result for result in results_ransac if result is not None]\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.results_icp = []\n",
    "        \n",
    "    #local refinement\n",
    "    def refine_registration(self):\n",
    "        print(\":: Point-to-plane ICP registration is applied on original point\")\n",
    "        print(\"   clouds to refine the alignment. This time we use a strict\")\n",
    "        print(\"   distance threshold %.3f.\" % self.distance_threshold)\n",
    "        for transform in self.results_ransac:\n",
    "            if transform is None:\n",
    "                continue\n",
    "            else:\n",
    "                result_icp = o3d.pipelines.registration.registration_icp(\n",
    "                    self.source, self.target, self.distance_threshold, transform,\n",
    "                    o3d.pipelines.registration.TransformationEstimationPointToPlane(),\n",
    "                    o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=30))\n",
    "                self.results_icp.append(result_icp)\n",
    "            \n",
    "    def draw_registration_result(self, transforms, mask_):\n",
    "        visualizer = []\n",
    "        self.target.paint_uniform_color([0, 0.651, 0.929])\n",
    "        visualizer.append(self.target)\n",
    "        if len(transforms)<1:\n",
    "            print(\"ICP not implemented. Cannot visualize\")\n",
    "            return \n",
    "        else:\n",
    "            for i, transform in enumerate(transforms):\n",
    "                if mask_[i] == True:\n",
    "                    source_temp = copy.deepcopy(self.source)\n",
    "                    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "                    source_temp.transform(transform)\n",
    "                    obb = source_temp.get_oriented_bounding_box()\n",
    "                    obb.color = (0,1,0) #obbBounding box is green\n",
    "                    visualizer.append(source_temp)\n",
    "                    visualizer.append(obb)\n",
    "            o3d.visualization.draw_geometries(visualizer,\n",
    "                                          zoom=0.4559,\n",
    "                                          front=[0.6452, -0.3036, -0.7011],\n",
    "                                          lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                          up=[-0.2779, -0.9482, 0.1556])\n",
    "            \n",
    "    def get_model_instances(self, transforms):\n",
    "        instances = []\n",
    "        if len(transforms)<1:\n",
    "            print(\"No transforms found\")\n",
    "            return \n",
    "        else:\n",
    "            for transform in transforms:\n",
    "                source_temp = copy.deepcopy(self.source)\n",
    "                source_temp.transform(transform)\n",
    "                instances.append(source_temp)\n",
    "        return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy_verification.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hypothesisVerification():\n",
    "    __slots__ = ('mask_', 'scene_cloud_', 'occlusion_cloud_', 'scene_cloud_downsampled_', \n",
    "                 'scene_downsampled_tree_', 'visible_models_', 'visible_normal_models_', \n",
    "                 'resolution_', 'inliers_threshold_', 'occlusion_thres_', 'requires_normals_', \n",
    "                 'normals_set_', 'zbuffer_scene_resolution_', 'zbuffer_self_occlusion_resolution_',\n",
    "                 'occlusion_cloud_set_', 'complete_models_')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zbuffer_scene_resolution_ = 100\n",
    "        self.zbuffer_self_occlusion_resolution_ = 150\n",
    "        self.resolution_ = 0.5\n",
    "        self.inliers_threshold_ = 1.0  # self.resolution_\n",
    "        self.occlusion_cloud_set_ = False\n",
    "        self.occlusion_thres_ = 0.05\n",
    "        self.normals_set_ = False\n",
    "        self.requires_normals_ = False\n",
    "        \n",
    "    def copyPointCloud(self, cloud_in, indices):\n",
    "        cloud_out = o3d.geometry.PointCloud()\n",
    "        cloud_out.points = o3d.utility.Vector3dVector(np.asarray(cloud_in.points)[indices])\n",
    "        return cloud_out\n",
    "        \n",
    "    def addNormalsClouds(self, complete_models):\n",
    "        complete_normal_models_ = complete_models\n",
    "        self.normals_set_ = True\n",
    "    \n",
    "    def addModels(self, models, occlusion_reasoning_=False):\n",
    "        self.mask_ = []\n",
    "        self.visible_models_ = []\n",
    "        self.visible_normal_models_ = []\n",
    "        \n",
    "        if not self.occlusion_cloud_set_:\n",
    "            print('occlusion cloud not set, using scene_cloud instead...')\n",
    "            self.occlusion_cloud_ = self.scene_cloud_\n",
    "            \n",
    "        if not occlusion_reasoning_:\n",
    "            self.visible_models_ = models\n",
    "            \n",
    "        else:\n",
    "            if (self.scene_cloud_ == None):\n",
    "                print(\"Scene cloud should be set before adding the model if reasoning about occlusions...\")\n",
    "            \n",
    "            zbuffer_scene = occlusion_reasoning.ZBuffering(self.zbuffer_scene_resolution_, self.zbuffer_scene_resolution_, 1.)\n",
    "            zbuffer_scene.computeDepthMap(self.occlusion_cloud_, True)\n",
    "            #print(\"models: \", len(models))\n",
    "            for i in range(len(models)):\n",
    "                #self occlusions\n",
    "                #print(\"models: \", models[i])\n",
    "                zbuffer_self_occlusion = occlusion_reasoning.ZBuffering(75, 75, 1.)\n",
    "                zbuffer_self_occlusion.computeDepthMap(models[i], True)\n",
    "                zbuffer_self_occlusion.filter_with_indices(models[i], 0.005)\n",
    "                filtered = self.copyPointCloud(models[i], zbuffer_self_occlusion.indices_to_keep)\n",
    "                \n",
    "                if self.normals_set_ and self.requires_normals_:\n",
    "                    filtered_normals = self.copyPointCloud(self.complete_normal_models_[i], zbuffer_self_occlusion.indices_to_keep)\n",
    "                    self.visible_normal_models_.append(filtered_normals)\n",
    "                \n",
    "                ##scene occlusions\n",
    "                zbuffer_scene.Filter(filtered, filtered, self.occlusion_thres_)\n",
    "                self.visible_models_.append(filtered)\n",
    "                \n",
    "            self.complete_models_ = models\n",
    "            \n",
    "        self.occlusion_cloud_set_ = False\n",
    "        self.normals_set_ = False\n",
    "        \n",
    "    #sets the scene cloud scene_cloud point cloud representing the scene \n",
    "    def setSceneCloud(self, scene_cloud):\n",
    "        self.complete_models_ = []\n",
    "        self.visible_models_ = []\n",
    "        self.visible_normal_models_ = []\n",
    "        \n",
    "        self.scene_cloud_ = scene_cloud\n",
    "        self.scene_cloud_downsampled_ = scene_cloud.voxel_down_sample(self.resolution_)\n",
    "        \n",
    "        #initialize kdTree for search\n",
    "        self.scene_downsampled_tree_ = o3d.geometry.KDTreeFlann(self.scene_cloud_downsampled_)\n",
    "        \n",
    "    def setOcclusionCloud(self, occ_cloud):\n",
    "        self.occlusion_cloud_ = occ_cloud\n",
    "        self.occlusion_cloud_set_ = True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class greedyHypothesisVerification(hypothesisVerification):\n",
    "    def __init__(self, source, model, model_instances):\n",
    "        \n",
    "        #initialize hypothesisVerification superclass and set scene and model instances\n",
    "        super().__init__()\n",
    "        super().setSceneCloud(source)\n",
    "        super().addModels(model_instances, True)\n",
    "\n",
    "        #brief recognition models and indices\n",
    "        self.indices_models_ = []\n",
    "\n",
    "        #brief recognition models(hypotheses to be verified)\n",
    "        self.recognition_models_ = []\n",
    "\n",
    "        #brief recognition models that explain a scene point\n",
    "        self.points_explained_by_rm = []\n",
    "\n",
    "        #brief weighting for outliers\n",
    "        self.regularizer_ = 1.0\n",
    "\n",
    "        #self.visible_models_ = []\n",
    "        self.radius = 1.0\n",
    "\n",
    "        #initialize mask\n",
    "        #self.mask_ = [None for i in self.visible_models_] \n",
    "        self.mask_ = [False for model in self.visible_models_]\n",
    "\n",
    "        #initialize explained_by_rm\n",
    "        self.points_explained_by_rm = [[] for i in self.scene_cloud_downsampled_.points]\n",
    "\n",
    "        #initialize model\n",
    "        for m, model_ in enumerate(self.visible_models_):\n",
    "            recog_model = self.recognitionModel()  \n",
    "            recog_model.id_ = m\n",
    "            recog_model.cloud = self.visible_models_[m].voxel_down_sample(self.resolution_)\n",
    "\n",
    "            explained_indices = []\n",
    "            outliers = []\n",
    "            nn_indices = []\n",
    "            nn_distances = []\n",
    "            \n",
    "            cloud_pts = recog_model.cloud.points\n",
    "            \n",
    "            for i in range(len(cloud_pts)):\n",
    "                \n",
    "                [num, idx, sqDist] = self.scene_downsampled_tree_.search_radius_vector_xd(cloud_pts[i], self.inliers_threshold_)\n",
    "                \n",
    "                nn_indices = idx\n",
    "                nn_distances = sqDist\n",
    "                \n",
    "                if not num:\n",
    "                    outliers.append(i)\n",
    "                    \n",
    "                else:\n",
    "                    for k in range(len(nn_distances)):\n",
    "                        explained_indices.append(nn_indices[k])\n",
    "                        \n",
    "            #print(explained_indices)\n",
    "            \n",
    "            explained_indices = list(set(explained_indices))\n",
    "\n",
    "            recog_model.bad_information_ = len(outliers)\n",
    "            recog_model.explained_ = explained_indices\n",
    "            recog_model.good_information_ = len(explained_indices)\n",
    "            recog_model.regularizer_ = self.regularizer_\n",
    "\n",
    "            self.recognition_models_.append(recog_model)\n",
    "\n",
    "            for explained_index in explained_indices:\n",
    "                #print(\"explained_index: \",explained_index)\n",
    "                self.points_explained_by_rm[explained_index].append(recog_model)\n",
    "\n",
    "        self.sortModels()\n",
    "\n",
    "\n",
    "    #brief recognition model used during the verification\n",
    "    class recognitionModel():\n",
    "        __slots__ = ('explained_', 'cloud', 'bad_information_', 'good_information_', 'id_', 'regularizer_')\n",
    "\n",
    "\n",
    "    #sorts recognition models based on the number of explained scene points and visible outliers\n",
    "    def sortModelsClass(self, recogModel):\n",
    "        val = recogModel.good_information_ - recogModel.bad_information_*recogModel.regularizer_ \n",
    "        return val\n",
    "\n",
    "    #brief recognition model indices to keep track of the sorted recognition hypotheses     \n",
    "    class modelIndices(object):\n",
    "        __slots__ = ('index', 'model_')\n",
    "\n",
    "    #sort model indices similar to sortModelsClass \n",
    "    def sortModelIndicesClass(self, modelIndices):\n",
    "        val = modelIndices.model_.good_information_ - modelIndices.model_.bad_information_ * modelIndices.model_.regularizer_\n",
    "        return val\n",
    "\n",
    "    #brief sorts the hypotheses for the greedy approach\n",
    "    def sortModels(self):\n",
    "        self.indices_models_ = []\n",
    "        for i, model in enumerate(self.recognition_models_):\n",
    "            mi = self.modelIndices\n",
    "            mi.index = i\n",
    "            mi.model_ = model\n",
    "            self.indices_models_.append(mi)\n",
    "\n",
    "        self.indices_models_.sort(key = lambda modelIndices : self.sortModelIndicesClass(modelIndices))\n",
    "        self.recognition_models_.sort(key = lambda recognitionModel : self.sortModelsClass(recognitionModel))\n",
    "\n",
    "    #updates conflicting recognition hypotheses when a hypothesis is accepted\n",
    "    def updateGoodInformation(self, i):\n",
    "        for k in range(len(self.recognition_models_[i].explained_)):\n",
    "            for kk in range(len(self.points_explained_by_rm[self.recognition_models_[i].explained_[k]])):\n",
    "                (self.points_explained_by_rm[self.recognition_models_[i].explained_[k]])[kk].good_information_-=1 \n",
    "                (self.points_explained_by_rm[self.recognition_models_[i].explained_[k]])[kk].bad_information_ +=1\n",
    "\n",
    "    #starts verification\n",
    "    def verify(self):\n",
    "        best_solution_ = []\n",
    "        #best_solution_ = [False for i in self.recognition_models_]\n",
    "        \n",
    "        for i in range(len(self.recognition_models_)):\n",
    "            if self.recognition_models_[i].good_information_ > self.regularizer_ * self.recognition_models_[i].bad_information_:\n",
    "                best_solution_.append(True)\n",
    "                self.updateGoodInformation(i)\n",
    "            else:\n",
    "                best_solution_.append(False)\n",
    "\n",
    "        for i in range(len(best_solution_)):\n",
    "            if best_solution_[i]:\n",
    "                self.mask_[self.indices_models_[i].index] = True\n",
    "            else:\n",
    "                self.mask_[self.indices_models_[i].index] = False   \n",
    "        #print(best_solution_)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class occlusion_reasoning():\n",
    "    class ZBuffering():\n",
    "        __slots__ = ('f_', 'cx_', 'cy_', 'depth_', 'indices_to_keep', 'filtered')\n",
    "        \n",
    "        def __init__(self, resx, resy, f):\n",
    "            self.f_  = f\n",
    "            self.cx_ = resx\n",
    "            self.cy_ = resy\n",
    "            self.depth_ = None\n",
    "            \n",
    "            \n",
    "        def computeDepthMap(self, scene, compute_focal = False, smooth = False, wsize = 3):\n",
    "            cx = self.cx_ /2.0 - 0.5\n",
    "            cy = self.cy_ /2.0 - 0.5            \n",
    "            \n",
    "            if compute_focal:\n",
    "                max_u = max_v = -1*(sys.maxsize)\n",
    "                min_u = min_v = sys.maxsize \n",
    "                \n",
    "                for point in scene.points:\n",
    "                    b_x = point[0]/point[2]\n",
    "                    if b_x > max_u:\n",
    "                        max_u = b_x\n",
    "                    if b_x < min_u:\n",
    "                        min_u = b_x\n",
    "                        \n",
    "                    b_y = point[1]/point[2]\n",
    "                    if b_y > max_v:\n",
    "                        max_v = b_y\n",
    "                    if b_y < min_v:\n",
    "                        min_v = b_y \n",
    "                        \n",
    "                maxC = np.maximum(np.maximum(np.abs(max_u), np.abs(max_v)), np.maximum(np.abs(min_u), np.abs(min_v)))\n",
    "                self.f_ = cx / maxC\n",
    "                #print(max_u, min_u, max_v, min_v, self.f_, len(scene.points))\n",
    "                \n",
    "            self.depth_ = np.zeros(self.cx_*self.cy_)\n",
    "            self.depth_[:] = np.NaN\n",
    "            \n",
    "            for point in scene.points:\n",
    "                x, y, z = point[0], point[1], point[2]\n",
    "                u, v = int(self.f_ * x / z + cx), int(self.f_ * y / z + cy)\n",
    "                \n",
    "                if u >= self.cx_ or v >= self.cy_ or u < 0  or v < 0:\n",
    "                    continue\n",
    "                    \n",
    "                #print(x, y, z, self.f_, cx, cy, u, v, (self.f_ * x / z) )   \n",
    "                \n",
    "                if x < self.depth_[u*self.cy_+v] or not np.isfinite(self.depth_[u*self.cy_+v]):\n",
    "                    #print(u*self.cx_+v, z)\n",
    "                    self.depth_[u*self.cx_+v] = z\n",
    "                    \n",
    "            if smooth:\n",
    "                ws = wsize\n",
    "                ws2 = int(np.floor(ws/2.0))\n",
    "                depth_smooth = np.zeros(self.cx_*self.cy_)\n",
    "                for i in range(self.cx_*self.cy_):\n",
    "                    depth_smooth[i] = np.NaN\n",
    "                    \n",
    "                for u in range(ws2, self.cx_ - ws2):\n",
    "                    for v in range(ws2, self.cy_ - ws2):\n",
    "                        min_ = sys.maxsize\n",
    "                        for j in range(u-ws2, u+ws2):\n",
    "                            for i in range(v-ws2, v+ws2):\n",
    "                                if np.isfinite(self.depth_[j*self.cx_+i]) and self.depth_[j*self.cx_+i] < min_:\n",
    "                                    min_ = self.depth_[j*self.cx_+i]\n",
    "                \n",
    "                        if min_ < sys.maxsize - 0.1:\n",
    "                            depth_smooth[u*self.cx_+v] = min_\n",
    "                            \n",
    "                self.depth_ = depth_smooth\n",
    "                \n",
    "        def copyPointCloud(self, cloud_in, indices):\n",
    "            cloud_out = o3d.geometry.PointCloud()\n",
    "            cloud_out.points = o3d.utility.Vector3dVector(np.asarray(cloud_in.points)[indices])\n",
    "            return cloud_out \n",
    "        \n",
    "        def Filter(self, model, filtered, thres):  \n",
    "            self.filter_with_indices(model, thres)\n",
    "            filtered = self.copyPointCloud(model, self.indices_to_keep)\n",
    "            \n",
    "        def filter_with_indices(self, model, thres):\n",
    "            \n",
    "            cx = self.cx_/2.0 - 0.5\n",
    "            cy = self.cy_/2.0 - 0.5\n",
    "            \n",
    "            model_size = len(model.points)\n",
    "            self.indices_to_keep = []#self.indices_to_keep[:model_size]\n",
    "            keep = 0\n",
    "            \n",
    "            for i in range(model_size):\n",
    "                x = model.points[i][0]\n",
    "                y = model.points[i][1]\n",
    "                z = model.points[i][2]\n",
    "                u = int(self.f_ * x / z + cx)\n",
    "                v = int(self.f_ * y / z + cy)\n",
    "                \n",
    "                if u >= self.cx_ or v >= self.cy_ or u < 0 or v < 0:\n",
    "                    continue\n",
    "                    \n",
    "                if ((z-thres) > self.depth_[u*self.cy_ + v] or not np.isfinite(self.depth_[u*self.cy_ + v])):\n",
    "                    continue\n",
    "                    \n",
    "                #print(self.indices_to_keep, keep)    \n",
    "                #self.indices_to_keep[keep] = i \n",
    "                self.indices_to_keep.append(i) \n",
    "                keep +=1\n",
    "            \n",
    "            self.indices_to_keep = self.indices_to_keep[:keep]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pointclouds\n",
    "start = timer()\n",
    "voxel_size = 0.5\n",
    "source_input, target_input = read_pcds(\"tempsensor_pcd/temp_sensor2.pcd\", \"tempsensor_pcd/5.pcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess pointclouds (source)\n",
    "source, source_fpfh = preprocess_point_cloud(source_input, voxel_size, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess pointclouds (target)\n",
    "target, target_fpfh = preprocess_point_cloud(target_input, voxel_size*3, True, False)\n",
    "source.paint_uniform_color([1, 0.706, 0])\n",
    "target.paint_uniform_color([0, 0.651, 0.929])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_fpfh.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find correspondences\n",
    "findCorresp1 = findCorrespondences(source, target, source_fpfh, target_fpfh, 1, 105000)\n",
    "correspondences1, points1 = findCorresp1.run()\n",
    "findCorresp1.visualize()\n",
    "print(len(correspondences1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find neighbours of matched correspondences in 3d space\n",
    "target_tree = o3d.geometry.KDTreeFlann(target)\n",
    "indexs = []\n",
    "for i in range(len(correspondences1)):\n",
    "    [_, idx, _] = target_tree.search_radius_vector_3d(np.array(points1[1::2])[i], 10.)\n",
    "    #idx = sorted(idx)\n",
    "    indexs += idx[-10:]\n",
    "print(\"found: \", len(indexs))    \n",
    "indexs = list(set(indexs))\n",
    "print(\"unique: \", len(indexs))    \n",
    "\n",
    "# get fpfh at all points\n",
    "tgt = o3d.geometry.PointCloud()\n",
    "tgt.points = o3d.utility.Vector3dVector(np.asarray(target.points)[indexs])\n",
    "tgt_fpfh = o3d.pipelines.registration.Feature()\n",
    "tgt_fpfh.data = target_fpfh.data[:, indexs]\n",
    "tgt.paint_uniform_color((1,0,0))\n",
    "o3d.visualization.draw_geometries([target, tgt])\n",
    "print(len(tgt.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the neighbourhood of the found target points search knn features in source pointcloud\n",
    "findCorresp2 = findCorrespondences(tgt, source, tgt_fpfh, source_fpfh, 1, 150)\n",
    "correspondences2, points2 = findCorresp2.run()\n",
    "findCorresp2.visualize()\n",
    "\n",
    "correspondence_tmp = []\n",
    "\n",
    "for corr in correspondences2:\n",
    "    corr2 = correspondence(corr.match, corr.query, corr.distance)\n",
    "    correspondence_tmp.append(corr2)\n",
    "    \n",
    "correspondences = correspondences1 + correspondence_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(correspondences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find geometric consistent instances\n",
    "transforms, clustered_corrs = [], []\n",
    "gc = geometricConsistency(source, target, correspondences)\n",
    "gc.setGCThreshold(10)\n",
    "gc.setGCSize(10.0)\n",
    "gc.recognize(transforms, clustered_corrs)\n",
    "print(len(clustered_corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the aligned model instances based on geometric consistency\n",
    "results_ransac = gc.found_transformations_\n",
    "align = Alignment(results_ransac, source, target, 100)\n",
    "model_instances = align.get_model_instances(align.results_ransac)\n",
    "len(model_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy hv\n",
    "ghv = greedyHypothesisVerification(target, source, model_instances) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghv.verify() \n",
    "hypotheses_mask = ghv.mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ghv.mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ghv.mask_)):\n",
    "    if(hypotheses_mask[i]):\n",
    "        print(\"Instance %d is GOOD\"%(i))\n",
    "    else:\n",
    "        print(\"Instance %d is BAD\"%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_instances = [clustered_corrs[i] for i in range(len(ghv.mask_))] # if ghv.mask_[i] == True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[print(corr.query, corr.match) for corr in clustered_corrs[0]]\n",
    "#End algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize each instance of geometric consistency result\n",
    "\n",
    "instance = clustered_corrs[3]\n",
    "tgt = o3d.geometry.PointCloud()\n",
    "tgt.points = o3d.utility.Vector3dVector(np.array([np.array(target.points)[corr.match] for corr in instance]))\n",
    "spheres = []\n",
    "for pt in tgt.points:\n",
    "    sph = o3d.geometry.TriangleMesh.create_sphere(radius = 0.5)\n",
    "    sph.translate(pt)\n",
    "    sph.paint_uniform_color([1, 0, 0])\n",
    "    spheres.append(sph)\n",
    "    \n",
    "o3d.visualization.draw_geometries([target]+spheres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize all instances of geometric consistency result with similar colored lines\n",
    "\n",
    "clrs = [list(np.random.rand(3)) for i in verified_instances]\n",
    "def make_lineset(corrs, source, target, index):\n",
    "    points = []\n",
    "    lines = []\n",
    "    colors = []\n",
    "    for i, corr in enumerate(corrs):\n",
    "        points.append(np.asarray(source.points)[corr.query])\n",
    "        points.append(np.asarray(target.points)[corr.match])\n",
    "        lines.append([2*i, 2*i+1])\n",
    "        colors.append(clrs[index])\n",
    "\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines  = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return line_set\n",
    "\n",
    "def visualize_model(source, target, linesets):\n",
    "    o3d.visualization.draw_geometries([source, target] + linesets)\n",
    "\n",
    "numModels = len(verified_instances)\n",
    "print(numModels)\n",
    "linesets = [make_lineset(verified_instances[i], source, target, i) for i in range(numModels)]\n",
    "visualize_model(source, target, linesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visaulize matched instances with the target\n",
    "results_ransac = gc.found_transformations_\n",
    "align = Alignment(results_ransac, source, target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "align.refine_registration()\n",
    "end = timer()\n",
    "print(end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize verified models\n",
    "ghv.mask_ = [True for i in ghv.mask_]\n",
    "print(len(clustered_corrs))\n",
    "\n",
    "def getVec2iVec(correspondences):\n",
    "    arr = np.array([[corresp.query, corresp.match] for corresp in correspondences])\n",
    "    return o3d.utility.Vector2iVector(arr)\n",
    "\n",
    "t = []\n",
    "for i in range(len(ghv.mask_)):\n",
    "    correspVec2iVec = getVec2iVec(clustered_corrs[i])\n",
    "    ransac = o3d.pipelines.registration.registration_ransac_based_on_correspondence(source, target, correspVec2iVec, 0.5 )\n",
    "    t.append(ransac.transformation)\n",
    "\n",
    "align.draw_registration_result(t, ghv.mask_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghv.mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbuffering\n",
    "a = occlusion_reasoning.ZBuffering(500, 500, 420)\n",
    "a.computeDepthMap(target, True, True)\n",
    "d = a.depth_\n",
    "d[d>5000] = np.NaN\n",
    "print(d.shape)\n",
    "plt.imshow(d.reshape(500,500).transpose(1,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each cluster, find more neighbouring points (from model to matched points)\n",
    "len(clustered_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
